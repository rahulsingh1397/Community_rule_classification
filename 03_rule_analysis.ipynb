{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Rule Analysis - Extracting Transferable Violation Patterns\n",
    "\n",
    "This notebook performs an in-depth analysis of the 2 training rules to extract patterns that can generalize to unseen rules in the test set.\n",
    "\n",
    "## Goals:\n",
    "1. **Rule Characterization**: Deep dive into each rule's violation patterns\n",
    "2. **Cross-Rule Pattern Discovery**: Find common violation indicators\n",
    "3. **Transferable Feature Extraction**: Identify rule-agnostic features\n",
    "4. **Semantic Analysis**: Understand the underlying violation concepts\n",
    "5. **Generalization Strategy**: Design features for unseen rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# NLP libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Visualization\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2029, 9)\n",
      "Columns: ['row_id', 'body', 'rule', 'subreddit', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2', 'rule_violation']\n",
      "\n",
      "Training Rules (2):\n",
      "1. No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "2. No legal advice: Do not offer or request legal advice.\n",
      "\n",
      "Basic Statistics:\n",
      "No Advertising: Spam, referral...: 1012 samples, 43.3% violations\n",
      "No legal advice: Do not offer ...: 1017 samples, 58.3% violations\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train_df = pd.read_csv('Data/train.csv')\n",
    "\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Display the two rules\n",
    "rules = train_df['rule'].unique()\n",
    "print(f\"\\nTraining Rules ({len(rules)}):\")\n",
    "for i, rule in enumerate(rules, 1):\n",
    "    print(f\"{i}. {rule}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "for rule in rules:\n",
    "    rule_data = train_df[train_df['rule'] == rule]\n",
    "    violation_rate = rule_data['rule_violation'].mean()\n",
    "    print(f\"{rule[:30]}...: {len(rule_data)} samples, {violation_rate:.1%} violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rule-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYSIS: No Advertising: Spam, referral links, un...\n",
      "============================================================\n",
      "Violations: 438, Non-violations: 574\n",
      "\n",
      "Text Length Patterns:\n",
      "  Violation avg: 155.5 chars\n",
      "  Non-violation avg: 139.3 chars\n",
      "  Difference: 16.2 chars\n",
      "\n",
      "Word Count Patterns:\n",
      "  Violation avg: 21.1 words\n",
      "  Non-violation avg: 15.4 words\n",
      "\n",
      "Sentiment Patterns:\n",
      "  Violation sentiment: 0.383\n",
      "  Non-violation sentiment: 0.221\n",
      "\n",
      "============================================================\n",
      "ANALYSIS: No legal advice: Do not offer or request...\n",
      "============================================================\n",
      "Violations: 593, Non-violations: 424\n",
      "\n",
      "Text Length Patterns:\n",
      "  Violation avg: 225.0 chars\n",
      "  Non-violation avg: 182.3 chars\n",
      "  Difference: 42.7 chars\n",
      "\n",
      "Word Count Patterns:\n",
      "  Violation avg: 41.2 words\n",
      "  Non-violation avg: 33.6 words\n",
      "\n",
      "Sentiment Patterns:\n",
      "  Violation sentiment: -0.172\n",
      "  Non-violation sentiment: -0.196\n"
     ]
    }
   ],
   "source": [
    "def analyze_rule_patterns(rule_name, rule_data):\n",
    "    \"\"\"Comprehensive analysis of a specific rule's violation patterns\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYSIS: {rule_name[:40]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    violations = rule_data[rule_data['rule_violation'] == 1]\n",
    "    non_violations = rule_data[rule_data['rule_violation'] == 0]\n",
    "    \n",
    "    print(f\"Violations: {len(violations)}, Non-violations: {len(non_violations)}\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    viol_lengths = violations['body'].str.len()\n",
    "    non_viol_lengths = non_violations['body'].str.len()\n",
    "    \n",
    "    print(f\"\\nText Length Patterns:\")\n",
    "    print(f\"  Violation avg: {viol_lengths.mean():.1f} chars\")\n",
    "    print(f\"  Non-violation avg: {non_viol_lengths.mean():.1f} chars\")\n",
    "    print(f\"  Difference: {viol_lengths.mean() - non_viol_lengths.mean():.1f} chars\")\n",
    "    \n",
    "    # Word patterns\n",
    "    viol_words = violations['body'].str.split().str.len()\n",
    "    non_viol_words = non_violations['body'].str.split().str.len()\n",
    "    \n",
    "    print(f\"\\nWord Count Patterns:\")\n",
    "    print(f\"  Violation avg: {viol_words.mean():.1f} words\")\n",
    "    print(f\"  Non-violation avg: {non_viol_words.mean():.1f} words\")\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    viol_sentiment = [sia.polarity_scores(str(text))['compound'] \n",
    "                     for text in violations['body'] if pd.notna(text)]\n",
    "    non_viol_sentiment = [sia.polarity_scores(str(text))['compound'] \n",
    "                         for text in non_violations['body'] if pd.notna(text)]\n",
    "    \n",
    "    print(f\"\\nSentiment Patterns:\")\n",
    "    print(f\"  Violation sentiment: {np.mean(viol_sentiment):.3f}\")\n",
    "    print(f\"  Non-violation sentiment: {np.mean(non_viol_sentiment):.3f}\")\n",
    "    \n",
    "    return violations, non_violations\n",
    "\n",
    "# Analyze each rule\n",
    "rule_analyses = {}\n",
    "for rule in rules:\n",
    "    rule_data = train_df[train_df['rule'] == rule]\n",
    "    violations, non_violations = analyze_rule_patterns(rule, rule_data)\n",
    "    rule_analyses[rule] = {'violations': violations, 'non_violations': non_violations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for Transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting transferable features...\n",
      "Extracted 19 transferable features\n"
     ]
    }
   ],
   "source": [
    "def extract_transferable_features(text):\n",
    "    \"\"\"Extract features that should transfer across different rule types\"\"\"\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        text = \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    features = {\n",
    "        # Basic text statistics\n",
    "        'char_count': len(text),\n",
    "        'word_count': len(text.split()),\n",
    "        'sentence_count': len(re.findall(r'[.!?]+', text)),\n",
    "        \n",
    "        # Punctuation and formatting\n",
    "        'exclamation_count': text.count('!'),\n",
    "        'question_count': text.count('?'),\n",
    "        #'caps_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0,\n",
    "        # Fixed caps_ratio calculation:\n",
    "        'caps_ratio': sum(1 for c in str(text) if c.isupper()) / len(str(text)) if text else 0,\n",
    "        \n",
    "        # URLs and links\n",
    "        'url_count': len(re.findall(r'http[s]?://\\S+', text)),\n",
    "        'link_words': int(any(word in text for word in ['click', 'link', 'here', 'visit'])),\n",
    "        \n",
    "        # Commercial indicators\n",
    "        'commercial_words': int(any(word in text for word in ['buy', 'sell', 'price', 'cost', 'money', 'pay', 'free', 'discount', 'sale'])),\n",
    "        'promotional_words': int(any(word in text for word in ['offer', 'deal', 'special', 'limited', 'now', 'today'])),\n",
    "        \n",
    "        # Advice and instruction indicators\n",
    "        'advice_words': int(any(word in text for word in ['should', 'must', 'need', 'have to', 'recommend', 'suggest', 'advice'])),\n",
    "        'instruction_words': int(any(word in text for word in ['how to', 'step', 'guide', 'tutorial', 'instructions'])),\n",
    "        \n",
    "        # Legal and professional terms\n",
    "        'legal_words': int(any(word in text for word in ['lawyer', 'legal', 'court', 'law', 'sue', 'lawsuit', 'attorney'])),\n",
    "        'professional_words': int(any(word in text for word in ['professional', 'expert', 'consultant', 'service'])),\n",
    "        \n",
    "        # Spam indicators\n",
    "        'spam_words': int(any(word in text for word in ['spam', 'scam', 'fake', 'bot', 'automated'])),\n",
    "        'urgency_words': int(any(word in text for word in ['urgent', 'immediate', 'asap', 'quickly', 'hurry'])),\n",
    "        \n",
    "        # Special characters\n",
    "        'dollar_count': text.count('$'),\n",
    "        'percent_count': text.count('%'),\n",
    "        'number_count': len(re.findall(r'\\d+', text)),\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all comments\n",
    "print(\"Extracting transferable features...\")\n",
    "feature_list = []\n",
    "labels = []\n",
    "rules_list = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    features = extract_transferable_features(row['body'])\n",
    "    feature_list.append(features)\n",
    "    labels.append(row['rule_violation'])\n",
    "    rules_list.append(row['rule'])\n",
    "\n",
    "features_df = pd.DataFrame(feature_list)\n",
    "features_df['rule_violation'] = labels\n",
    "features_df['rule'] = rules_list\n",
    "\n",
    "print(f\"Extracted {len(features_df.columns)-2} transferable features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFERABLE FEATURES PREVIEW:\n",
      "==================================================\n",
      "\n",
      "Dataset shape: (2029, 21)\n",
      "Feature columns: ['char_count', 'word_count', 'sentence_count', 'exclamation_count', 'question_count', 'caps_ratio', 'url_count', 'link_words', 'commercial_words', 'promotional_words', 'advice_words', 'instruction_words', 'legal_words', 'professional_words', 'spam_words', 'urgency_words', 'dollar_count', 'percent_count', 'number_count']\n",
      "\n",
      "First 5 rows of extracted features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>caps_ratio</th>\n",
       "      <th>url_count</th>\n",
       "      <th>link_words</th>\n",
       "      <th>commercial_words</th>\n",
       "      <th>promotional_words</th>\n",
       "      <th>...</th>\n",
       "      <th>instruction_words</th>\n",
       "      <th>legal_words</th>\n",
       "      <th>professional_words</th>\n",
       "      <th>spam_words</th>\n",
       "      <th>urgency_words</th>\n",
       "      <th>dollar_count</th>\n",
       "      <th>percent_count</th>\n",
       "      <th>number_count</th>\n",
       "      <th>rule_violation</th>\n",
       "      <th>rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>No legal advice: Do not offer or request legal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>313</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No Advertising: Spam, referral links, unsolici...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   char_count  word_count  sentence_count  exclamation_count  question_count  \\\n",
       "0          59          12               2                  2               0   \n",
       "1          91           7               2                  0               0   \n",
       "2          57          12               2                  0               0   \n",
       "3          75          12               2                  0               0   \n",
       "4         313          23              10                  0               2   \n",
       "\n",
       "   caps_ratio  url_count  link_words  commercial_words  promotional_words  \\\n",
       "0         0.0          0           1                 0                  1   \n",
       "1         0.0          1           1                 0                  0   \n",
       "2         0.0          0           0                 0                  0   \n",
       "3         0.0          1           0                 0                  0   \n",
       "4         0.0          3           1                 1                  0   \n",
       "\n",
       "   ...  instruction_words  legal_words  professional_words  spam_words  \\\n",
       "0  ...                  0            0                   0           0   \n",
       "1  ...                  0            0                   0           0   \n",
       "2  ...                  0            0                   0           0   \n",
       "3  ...                  0            0                   0           0   \n",
       "4  ...                  0            0                   0           0   \n",
       "\n",
       "   urgency_words  dollar_count  percent_count  number_count  rule_violation  \\\n",
       "0              0             0              0             0               0   \n",
       "1              0             0              0             2               0   \n",
       "2              0             0              0             0               1   \n",
       "3              0             0              0             1               1   \n",
       "4              0             1              0            10               1   \n",
       "\n",
       "                                                rule  \n",
       "0  No Advertising: Spam, referral links, unsolici...  \n",
       "1  No Advertising: Spam, referral links, unsolici...  \n",
       "2  No legal advice: Do not offer or request legal...  \n",
       "3  No Advertising: Spam, referral links, unsolici...  \n",
       "4  No Advertising: Spam, referral links, unsolici...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>caps_ratio</th>\n",
       "      <th>url_count</th>\n",
       "      <th>link_words</th>\n",
       "      <th>commercial_words</th>\n",
       "      <th>promotional_words</th>\n",
       "      <th>advice_words</th>\n",
       "      <th>instruction_words</th>\n",
       "      <th>legal_words</th>\n",
       "      <th>professional_words</th>\n",
       "      <th>spam_words</th>\n",
       "      <th>urgency_words</th>\n",
       "      <th>dollar_count</th>\n",
       "      <th>percent_count</th>\n",
       "      <th>number_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.0</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.00000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "      <td>2029.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>176.843765</td>\n",
       "      <td>27.963036</td>\n",
       "      <td>3.111878</td>\n",
       "      <td>0.255298</td>\n",
       "      <td>0.282405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473632</td>\n",
       "      <td>0.209463</td>\n",
       "      <td>0.194184</td>\n",
       "      <td>0.17792</td>\n",
       "      <td>0.168063</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>0.199606</td>\n",
       "      <td>0.018236</td>\n",
       "      <td>0.033021</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.040907</td>\n",
       "      <td>0.037457</td>\n",
       "      <td>1.186299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>113.625378</td>\n",
       "      <td>21.230214</td>\n",
       "      <td>2.193275</td>\n",
       "      <td>0.765296</td>\n",
       "      <td>0.619848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.722913</td>\n",
       "      <td>0.407026</td>\n",
       "      <td>0.395669</td>\n",
       "      <td>0.38254</td>\n",
       "      <td>0.374014</td>\n",
       "      <td>0.144062</td>\n",
       "      <td>0.399803</td>\n",
       "      <td>0.133835</td>\n",
       "      <td>0.178736</td>\n",
       "      <td>0.096338</td>\n",
       "      <td>0.242851</td>\n",
       "      <td>0.356092</td>\n",
       "      <td>2.535311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>238.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        char_count   word_count  sentence_count  exclamation_count  \\\n",
       "count  2029.000000  2029.000000     2029.000000        2029.000000   \n",
       "mean    176.843765    27.963036        3.111878           0.255298   \n",
       "std     113.625378    21.230214        2.193275           0.765296   \n",
       "min      51.000000     1.000000        0.000000           0.000000   \n",
       "25%      87.000000    11.000000        2.000000           0.000000   \n",
       "50%     138.000000    22.000000        3.000000           0.000000   \n",
       "75%     238.000000    39.000000        4.000000           0.000000   \n",
       "max     499.000000    97.000000       30.000000           9.000000   \n",
       "\n",
       "       question_count  caps_ratio    url_count   link_words  commercial_words  \\\n",
       "count     2029.000000      2029.0  2029.000000  2029.000000       2029.000000   \n",
       "mean         0.282405         0.0     0.473632     0.209463          0.194184   \n",
       "std          0.619848         0.0     0.722913     0.407026          0.395669   \n",
       "min          0.000000         0.0     0.000000     0.000000          0.000000   \n",
       "25%          0.000000         0.0     0.000000     0.000000          0.000000   \n",
       "50%          0.000000         0.0     0.000000     0.000000          0.000000   \n",
       "75%          0.000000         0.0     1.000000     0.000000          0.000000   \n",
       "max          9.000000         0.0     8.000000     1.000000          1.000000   \n",
       "\n",
       "       promotional_words  advice_words  instruction_words  legal_words  \\\n",
       "count         2029.00000   2029.000000        2029.000000  2029.000000   \n",
       "mean             0.17792      0.168063           0.021193     0.199606   \n",
       "std              0.38254      0.374014           0.144062     0.399803   \n",
       "min              0.00000      0.000000           0.000000     0.000000   \n",
       "25%              0.00000      0.000000           0.000000     0.000000   \n",
       "50%              0.00000      0.000000           0.000000     0.000000   \n",
       "75%              0.00000      0.000000           0.000000     0.000000   \n",
       "max              1.00000      1.000000           1.000000     1.000000   \n",
       "\n",
       "       professional_words   spam_words  urgency_words  dollar_count  \\\n",
       "count         2029.000000  2029.000000    2029.000000   2029.000000   \n",
       "mean             0.018236     0.033021       0.009364      0.040907   \n",
       "std              0.133835     0.178736       0.096338      0.242851   \n",
       "min              0.000000     0.000000       0.000000      0.000000   \n",
       "25%              0.000000     0.000000       0.000000      0.000000   \n",
       "50%              0.000000     0.000000       0.000000      0.000000   \n",
       "75%              0.000000     0.000000       0.000000      0.000000   \n",
       "max              1.000000     1.000000       1.000000      4.000000   \n",
       "\n",
       "       percent_count  number_count  \n",
       "count    2029.000000   2029.000000  \n",
       "mean        0.037457      1.186299  \n",
       "std         0.356092      2.535311  \n",
       "min         0.000000      0.000000  \n",
       "25%         0.000000      0.000000  \n",
       "50%         0.000000      0.000000  \n",
       "75%         0.000000      2.000000  \n",
       "max        10.000000     34.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature correlation with violations:\n",
      "\n",
      "Top 10 features correlated with violations:\n",
      " 1. caps_ratio          :    nan ↓\n",
      " 2. legal_words         :  0.346 ↑\n",
      " 3. word_count          :  0.224 ↑\n",
      " 4. char_count          :  0.167 ↑\n",
      " 5. url_count           : -0.163 ↓\n",
      " 6. commercial_words    :  0.119 ↑\n",
      " 7. dollar_count        :  0.109 ↑\n",
      " 8. promotional_words   :  0.089 ↑\n",
      " 9. sentence_count      :  0.058 ↑\n",
      "10. spam_words          :  0.055 ↑\n",
      "\n",
      "Sample feature values for violations vs non-violations:\n",
      "\n",
      "Violations (n=1031):\n",
      "char_count            195.483\n",
      "word_count             32.637\n",
      "sentence_count          3.238\n",
      "exclamation_count       0.281\n",
      "question_count          0.273\n",
      "caps_ratio              0.000\n",
      "url_count               0.358\n",
      "link_words              0.231\n",
      "commercial_words        0.241\n",
      "promotional_words       0.211\n",
      "advice_words            0.185\n",
      "instruction_words       0.024\n",
      "legal_words             0.336\n",
      "professional_words      0.025\n",
      "spam_words              0.043\n",
      "urgency_words           0.013\n",
      "dollar_count            0.067\n",
      "percent_count           0.036\n",
      "number_count            1.098\n",
      "dtype: float64\n",
      "\n",
      "Non-violations (n=998):\n",
      "char_count            157.588\n",
      "word_count             23.134\n",
      "sentence_count          2.982\n",
      "exclamation_count       0.228\n",
      "question_count          0.293\n",
      "caps_ratio              0.000\n",
      "url_count               0.593\n",
      "link_words              0.187\n",
      "commercial_words        0.146\n",
      "promotional_words       0.143\n",
      "advice_words            0.150\n",
      "instruction_words       0.018\n",
      "legal_words             0.059\n",
      "professional_words      0.011\n",
      "spam_words              0.023\n",
      "urgency_words           0.006\n",
      "dollar_count            0.014\n",
      "percent_count           0.039\n",
      "number_count            1.278\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print preview of new features\n",
    "print(\"TRANSFERABLE FEATURES PREVIEW:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nDataset shape: {features_df.shape}\")\n",
    "print(f\"Feature columns: {[col for col in features_df.columns if col not in ['rule_violation', 'rule']]}\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows of extracted features:\")\n",
    "display(features_df.head())\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "feature_cols = [col for col in features_df.columns if col not in ['rule_violation', 'rule']]\n",
    "display(features_df[feature_cols].describe())\n",
    "\n",
    "print(f\"\\nFeature correlation with violations:\")\n",
    "correlations = []\n",
    "for feature in feature_cols:\n",
    "    corr = features_df[feature].corr(features_df['rule_violation'])\n",
    "    correlations.append((feature, corr))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 features correlated with violations:\")\n",
    "for i, (feature, corr) in enumerate(correlations[:10]):\n",
    "    direction = \"↑\" if corr > 0 else \"↓\"\n",
    "    print(f\"{i+1:2d}. {feature:20s}: {corr:6.3f} {direction}\")\n",
    "\n",
    "print(f\"\\nSample feature values for violations vs non-violations:\")\n",
    "violations = features_df[features_df['rule_violation'] == 1]\n",
    "non_violations = features_df[features_df['rule_violation'] == 0]\n",
    "\n",
    "print(f\"\\nViolations (n={len(violations)}):\")\n",
    "print(violations[feature_cols].mean().round(3))\n",
    "\n",
    "print(f\"\\nNon-violations (n={len(non_violations)}):\")\n",
    "print(non_violations[feature_cols].mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Rule Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CROSS-RULE DISCRIMINATIVE FEATURES:\n",
      "==================================================\n",
      "\n",
      "Top 10 most discriminative features:\n",
      " 1. char_count          : Higher in violations (diff: 37.895)\n",
      " 2. word_count          : Higher in violations (diff: 9.503)\n",
      " 3. legal_words         : Higher in violations (diff: 0.276)\n",
      " 4. sentence_count      : Higher in violations (diff: 0.256)\n",
      " 5. url_count           : Lower  in violations (diff: 0.235)\n",
      " 6. number_count        : Lower  in violations (diff: 0.180)\n",
      " 7. commercial_words    : Higher in violations (diff: 0.094)\n",
      " 8. promotional_words   : Higher in violations (diff: 0.068)\n",
      " 9. dollar_count        : Higher in violations (diff: 0.053)\n",
      "10. exclamation_count   : Higher in violations (diff: 0.053)\n",
      "\n",
      "\n",
      "RULE-SPECIFIC FEATURE ANALYSIS:\n",
      "==================================================\n",
      "\n",
      "No Advertising: Spam, referral links, un...\n",
      "  Samples: 1012 (438 violations)\n",
      "  Top 5 discriminative features:\n",
      "    char_count          : Higher (16.203)\n",
      "    word_count          : Higher (5.668)\n",
      "    exclamation_count   : Higher (0.226)\n",
      "    commercial_words    : Higher (0.213)\n",
      "    url_count           : Lower  (0.191)\n",
      "\n",
      "No legal advice: Do not offer or request...\n",
      "  Samples: 1017 (593 violations)\n",
      "  Top 5 discriminative features:\n",
      "    char_count          : Higher (42.690)\n",
      "    word_count          : Higher (7.597)\n",
      "    sentence_count      : Higher (0.554)\n",
      "    legal_words         : Higher (0.453)\n",
      "    question_count      : Lower  (0.151)\n"
     ]
    }
   ],
   "source": [
    "# Analyze which features are most discriminative across both rules\n",
    "print(\"\\nCROSS-RULE DISCRIMINATIVE FEATURES:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "violations = features_df[features_df['rule_violation'] == 1]\n",
    "non_violations = features_df[features_df['rule_violation'] == 0]\n",
    "\n",
    "feature_importance = []\n",
    "\n",
    "for feature in features_df.columns:\n",
    "    if feature not in ['rule_violation', 'rule']:\n",
    "        viol_mean = violations[feature].mean()\n",
    "        non_viol_mean = non_violations[feature].mean()\n",
    "        difference = abs(viol_mean - non_viol_mean)\n",
    "        \n",
    "        feature_importance.append({\n",
    "            'feature': feature,\n",
    "            'violation_mean': viol_mean,\n",
    "            'non_violation_mean': non_viol_mean,\n",
    "            'difference': difference,\n",
    "            'direction': 'Higher' if viol_mean > non_viol_mean else 'Lower'\n",
    "        })\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance.sort(key=lambda x: x['difference'], reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 most discriminative features:\")\n",
    "for i, feat in enumerate(feature_importance[:10]):\n",
    "    print(f\"{i+1:2d}. {feat['feature']:20s}: {feat['direction']:6s} in violations (diff: {feat['difference']:.3f})\")\n",
    "\n",
    "# Analyze by rule\n",
    "print(\"\\n\\nRULE-SPECIFIC FEATURE ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for rule in rules:\n",
    "    print(f\"\\n{rule[:40]}...\")\n",
    "    \n",
    "    rule_data = features_df[features_df['rule'] == rule]\n",
    "    rule_violations = rule_data[rule_data['rule_violation'] == 1]\n",
    "    rule_non_violations = rule_data[rule_data['rule_violation'] == 0]\n",
    "    \n",
    "    print(f\"  Samples: {len(rule_data)} ({len(rule_violations)} violations)\")\n",
    "    \n",
    "    # Top features for this rule\n",
    "    rule_feature_importance = []\n",
    "    for feature in features_df.columns:\n",
    "        if feature not in ['rule_violation', 'rule']:\n",
    "            viol_mean = rule_violations[feature].mean()\n",
    "            non_viol_mean = rule_non_violations[feature].mean()\n",
    "            difference = abs(viol_mean - non_viol_mean)\n",
    "            rule_feature_importance.append((feature, difference, viol_mean, non_viol_mean))\n",
    "    \n",
    "    rule_feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"  Top 5 discriminative features:\")\n",
    "    for feature, diff, v_mean, nv_mean in rule_feature_importance[:5]:\n",
    "        direction = \"Higher\" if v_mean > nv_mean else \"Lower\"\n",
    "        print(f\"    {feature:20s}: {direction:6s} ({diff:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rule Examples Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RULE EXAMPLES ANALYSIS:\n",
      "==================================================\n",
      "\n",
      "Rule: No Advertising: Spam, referral links, unsolicited advertising, and promotional content are not allowed.\n",
      "Subreddit: Futurology\n",
      "\n",
      "  Positive Example 1:\n",
      "    Text: If you could tell your younger self something different about sex, what would that be?\n",
      "\n",
      "i AM IN A CO...\n",
      "    Key features: promotional_words, url_count\n",
      "\n",
      "  Positive Example 2:\n",
      "    Text: hunt for lady for jack off in neighbourhood http://url.inmusi.com/gakq...\n",
      "    Key features: url_count\n",
      "\n",
      "  Negative Example 1:\n",
      "    Text: Watch Golden Globe Awards 2017 Live Online in HD Coverage without ADS (VIP STREAMS)\n",
      "=\n",
      "\n",
      "HD STREAM QUA...\n",
      "    Key features: url_count\n",
      "\n",
      "  Negative Example 2:\n",
      "    Text: DOUBLE CEE x BANDS EPPS - \"BIRDS\"\n",
      "\n",
      "DOWNLOAD/STREAM:\n",
      "\n",
      "http://music.theblacksmithed.com/download/birds...\n",
      "    Key features: url_count\n",
      "\n",
      "Rule: No legal advice: Do not offer or request legal advice.\n",
      "Subreddit: pcmasterrace\n",
      "\n",
      "  Positive Example 1:\n",
      "    Text: Don't break up with him or call the cops.  If you are willing to get beat up by him to stay with him...\n",
      "    Key features: promotional_words, advice_words\n",
      "\n",
      "  Positive Example 2:\n",
      "    Text: It'll be dismissed: https://en.wikipedia.org/wiki/New_York_Times_Co._v._Sullivan\n",
      "\n",
      "The first amendmen...\n",
      "    Key features: legal_words, url_count\n",
      "\n",
      "  Negative Example 1:\n",
      "    Text: Where is there a site that still works where you can jump the GPS. Is there a FAQ to do this with iP...\n",
      "    Key features: None detected\n",
      "\n",
      "  Negative Example 2:\n",
      "    Text: Because this statement of his is true. It isn't freedom of the press, it's libel. And because of thi...\n",
      "    Key features: commercial_words\n"
     ]
    }
   ],
   "source": [
    "# Analyze the provided rule examples\n",
    "print(\"\\nRULE EXAMPLES ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "unique_rules = train_df.drop_duplicates(subset=['rule'])\n",
    "\n",
    "for _, row in unique_rules.iterrows():\n",
    "    rule = row['rule']\n",
    "    print(f\"\\nRule: {rule}\")\n",
    "    print(f\"Subreddit: {row['subreddit']}\")\n",
    "    \n",
    "    examples = {\n",
    "        'Positive Example 1': row['positive_example_1'],\n",
    "        'Positive Example 2': row['positive_example_2'],\n",
    "        'Negative Example 1': row['negative_example_1'],\n",
    "        'Negative Example 2': row['negative_example_2']\n",
    "    }\n",
    "    \n",
    "    for example_type, example_text in examples.items():\n",
    "        if pd.notna(example_text):\n",
    "            print(f\"\\n  {example_type}:\")\n",
    "            print(f\"    Text: {str(example_text)[:100]}...\")\n",
    "            \n",
    "            # Extract features from example\n",
    "            features = extract_transferable_features(example_text)\n",
    "            \n",
    "            # Show key features\n",
    "            key_features = ['commercial_words', 'promotional_words', 'advice_words', 'legal_words', 'url_count']\n",
    "            feature_summary = []\n",
    "            for feat in key_features:\n",
    "                if features[feat] > 0:\n",
    "                    feature_summary.append(feat)\n",
    "            \n",
    "            if feature_summary:\n",
    "                print(f\"    Key features: {', '.join(feature_summary)}\")\n",
    "            else:\n",
    "                print(f\"    Key features: None detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transferable Patterns Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRANSFERABLE PATTERNS SUMMARY:\n",
      "============================================================\n",
      "\n",
      "1. MOST IMPORTANT CROSS-RULE FEATURES:\n",
      "   1. char_count           - Higher in violations\n",
      "   2. word_count           - Higher in violations\n",
      "   3. legal_words          - Higher in violations\n",
      "   4. sentence_count       - Higher in violations\n",
      "   5. url_count            - Lower  in violations\n",
      "   6. number_count         - Lower  in violations\n",
      "   7. commercial_words     - Higher in violations\n",
      "   8. promotional_words    - Higher in violations\n",
      "\n",
      "2. VIOLATION INDICATORS THAT TRANSFER:\n",
      "   • char_count: Higher in violations\n",
      "   • word_count: Higher in violations\n",
      "   • legal_words: Higher in violations\n",
      "   • sentence_count: Higher in violations\n",
      "   • url_count: Lower in violations\n",
      "   • number_count: Lower in violations\n",
      "   • commercial_words: Higher in violations\n",
      "   • promotional_words: Higher in violations\n",
      "   • dollar_count: Higher in violations\n",
      "   • exclamation_count: Higher in violations\n",
      "\n",
      "3. RECOMMENDED FEATURES FOR UNSEEN RULES:\n",
      "   • commercial_words    : Higher (importance: 0.094)\n",
      "   • promotional_words   : Higher (importance: 0.068)\n",
      "   • advice_words        : Higher (importance: 0.035)\n",
      "   • legal_words         : Higher (importance: 0.276)\n",
      "   • url_count           : Lower  (importance: 0.235)\n",
      "   • link_words          : Higher (importance: 0.043)\n",
      "   • urgency_words       : Higher (importance: 0.007)\n",
      "   • char_count          : Higher (importance: 37.895)\n",
      "   • caps_ratio          : Lower  (importance: 0.000)\n",
      "\n",
      "4. GENERALIZATION STRATEGY:\n",
      "   • Focus on semantic content rather than rule-specific keywords\n",
      "   • Use commercial/promotional language as violation indicators\n",
      "   • Leverage text formatting patterns (caps, punctuation)\n",
      "   • Consider advice-giving language patterns\n",
      "   • Build rule embeddings from provided examples\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE - Ready for model building!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRANSFERABLE PATTERNS SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. MOST IMPORTANT CROSS-RULE FEATURES:\")\n",
    "top_features = feature_importance[:8]\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"   {i}. {feat['feature']:20s} - {feat['direction']:6s} in violations\")\n",
    "\n",
    "print(\"\\n2. VIOLATION INDICATORS THAT TRANSFER:\")\n",
    "transfer_indicators = []\n",
    "for feat in feature_importance[:15]:\n",
    "    if feat['difference'] > 0.05:  # Significant difference\n",
    "        transfer_indicators.append(feat)\n",
    "\n",
    "for indicator in transfer_indicators:\n",
    "    print(f\"   • {indicator['feature']}: {indicator['direction']} in violations\")\n",
    "\n",
    "print(\"\\n3. RECOMMENDED FEATURES FOR UNSEEN RULES:\")\n",
    "recommended_features = [\n",
    "    'commercial_words', 'promotional_words', 'advice_words', 'legal_words',\n",
    "    'url_count', 'link_words', 'urgency_words', 'char_count', 'caps_ratio'\n",
    "]\n",
    "\n",
    "for feature in recommended_features:\n",
    "    feat_info = next((f for f in feature_importance if f['feature'] == feature), None)\n",
    "    if feat_info:\n",
    "        print(f\"   • {feature:20s}: {feat_info['direction']:6s} (importance: {feat_info['difference']:.3f})\")\n",
    "\n",
    "print(\"\\n4. GENERALIZATION STRATEGY:\")\n",
    "print(\"   • Focus on semantic content rather than rule-specific keywords\")\n",
    "print(\"   • Use commercial/promotional language as violation indicators\")\n",
    "print(\"   • Leverage text formatting patterns (caps, punctuation)\")\n",
    "print(\"   • Consider advice-giving language patterns\")\n",
    "print(\"   • Build rule embeddings from provided examples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE - Ready for model building!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jigsaw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
